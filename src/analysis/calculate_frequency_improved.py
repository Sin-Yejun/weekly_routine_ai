import pandas as pd
import numpy as np
import os

def _drop_outliers(counts: pd.Series,
                   method: str = "iqr",          # "iqr" | "pct" | "mad" | "none"
                   strength: float = 1.5,        # iqr:k, pct:p(0~1), mad:z
                   hard_cap: int | None = None,
                   plan_cap: int | None = None) -> pd.Series:
    if counts.empty:
        return counts
    x = counts.copy().astype(float)
    upper = None
    if method == "iqr":
        q1, q3 = x.quantile([0.25, 0.75])
        iqr = q3 - q1
        upper = q3 + strength * iqr
    elif method == "pct":
        upper = x.quantile(float(strength))
    elif method == "mad":
        med = x.median()
        mad = np.median(np.abs(x - med)) or 1e-9
        z = 0.6745 * (x - med) / mad
        return x[np.abs(z) <= float(strength)]
    elif method == "none":
        pass
    else:
        raise ValueError("method must be one of {'iqr','pct','mad','none'}")

    caps = [c for c in [upper, hard_cap, plan_cap] if c is not None and np.isfinite(c)]
    if caps:
        cap = min(caps)
        x = x[x <= cap]
    return x

def calculate_frequency_sliding(input_path: str,
                                output_path: str,
                                window_days: int = 7,
                                min_sessions: int = 10,
                                exclude_zero: bool = True,
                                tie_break: str = "max",
                                outlier_method: str = "iqr",
                                outlier_strength: float = 1.5,
                                hard_cap: int | None = None,
                                plan_cap_map: dict | None = None,
                                plan_buffer: int = 0,
                                low_hist_probs=(0.05, 0.15, 0.25, 0.25, 0.15, 0.10, 0.05),
                                # --- ìƒˆ ì˜µì…˜: í†µê³„ ì €ì¥ ---
                                save_stats: bool = True):
    """
    date ê¸°ë°˜ 7ì¼ ìŠ¬ë¼ì´ë”© í•©ê³„ì˜ ëª¨ë“œ(ìµœë¹ˆê°’)ë¥¼ ê³„ì‚°.
    assigned_random == True/False ê°ê°ì— ëŒ€í•´ frequency ë¶„í¬/ìš”ì•½í†µê³„ë„ ì €ì¥.
    """

    if not os.path.exists(input_path):
        print(f"âŒ Error: Input file not found at {input_path}")
        return

    print(f"ğŸ”„ Loading workout data from {input_path}...")
    df = pd.read_parquet(input_path, columns=["user_id", "date"])
    print("âœ… Data loaded successfully.")

    if 'date' not in df.columns:
        raise ValueError("Input must contain a 'date' column.")
    df['date'] = pd.to_datetime(df['date'])

    user_session_counts = df['user_id'].value_counts()
    users_lt = user_session_counts[user_session_counts < min_sessions].index
    users_ge = user_session_counts[user_session_counts >= min_sessions].index

    results = []

    # < min_sessions â†’ í™•ë¥  ëœë¤(ì£¼1~7)
    if len(users_lt) > 0:
        print(f"ğŸ² Weighted random for {len(users_lt)} users (< {min_sessions} sessions)")
        freqs = np.random.choice([1,2,3,4,5,6,7], size=len(users_lt), p=np.array(low_hist_probs))
        res_lt = pd.DataFrame({
            'user_id': users_lt,
            'frequency': freqs.astype(int),
            'sessions_total': user_session_counts.loc[users_lt].values,
            'method': f'sliding_{window_days}d_mode',
            'outlier_method': outlier_method,
            'outlier_strength': outlier_strength,
            'tie_break': tie_break,
            'assigned_random': True
        })
        results.append(res_lt)

    # â‰¥ min_sessions â†’ ìŠ¬ë¼ì´ë”© + ì´ìƒì¹˜ í•„í„°ë§ + tie_break
    if len(users_ge) > 0:
        print(f"âš™ï¸ Sliding-window + outlier filtering for {len(users_ge)} users")
        out_rows = []
        df_ge = df[df['user_id'].isin(users_ge)]

        for uid, g in df_ge.groupby('user_id'):
            s = (
                g.sort_values('date')
                 .set_index('date')
                 .assign(val=1)['val']
                 .groupby(level=0).sum()
            )
            roll = s.rolling(f'{window_days}D').sum().astype(int)
            counts = roll[roll > 0] if exclude_zero else roll

            # ìœ ì €ë³„ ê³„íš ìƒí•œ(ìˆë‹¤ë©´)
            plan_cap = None
            if plan_cap_map and uid in plan_cap_map:
                plan_cap = int(plan_cap_map[uid]) + int(plan_buffer)

            counts_f = _drop_outliers(
                counts,
                method=outlier_method,
                strength=outlier_strength,
                hard_cap=hard_cap,
                plan_cap=plan_cap
            )

            if counts_f.empty:
                freq = np.random.choice([1,2,3,4,5,6,7], p=np.array(low_hist_probs))
                assigned_random = True
            else:
                vc = counts_f.value_counts()
                top = vc.max()
                modes = sorted(vc[vc == top].index)
                if tie_break == "min":
                    freq = int(min(modes))
                elif tie_break == "median":
                    freq = int(np.median(modes))
                else:  # "max"
                    freq = int(max(modes))
                assigned_random = False

            out_rows.append({
                'user_id': uid,
                'frequency': freq,
                'sessions_total': int(user_session_counts.loc[uid]),
                'method': f'sliding_{window_days}d_mode',
                'outlier_method': outlier_method,
                'outlier_strength': outlier_strength,
                'tie_break': tie_break,
                'assigned_random': assigned_random
            })

        results.append(pd.DataFrame(out_rows))

    if not results:
        print("âš ï¸ No user data to process.")
        return

    final_df = pd.concat(results, ignore_index=True)
    final_df['frequency'] = final_df['frequency'].astype(int)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    final_df.to_csv(output_path, index=False)

    print(f"\nğŸ‰ Calculated frequencies for {len(final_df)} users.")
    print(f"ğŸ’¾ Saved to {output_path}")
    print(final_df.head())

    # --------- ğŸ“Š í†µê³„ ì¶œë ¥/ì €ì¥ ---------
    def _summaries(df):
        # ìš”ì•½í†µê³„
        summary = (df.groupby('assigned_random')['frequency']
                     .agg(n_users='count',
                          mean='mean',
                          median='median',
                          std='std',
                          min='min',
                          max='max')
                     .reset_index())
        # ë¶„í¬(ë¹ˆë„í‘œ)
        dist = (df.groupby(['assigned_random','frequency'])
                  .size()
                  .rename('count')
                  .reset_index()
                  .sort_values(['assigned_random','frequency']))
        # ëˆ„ì ë¹„/ë¹„ìœ¨ ì¶”ê°€
        dist['ratio'] = dist['count'] / dist.groupby('assigned_random')['count'].transform('sum')
        dist['cum_ratio'] = dist.groupby('assigned_random')['ratio'].cumsum()
        return summary, dist

    summary, dist = _summaries(final_df)

    print("\n=== Summary by assigned_random ===")
    print(summary)
    print("\n=== Distribution by assigned_random & frequency ===")
    print(dist.head(30))

    if save_stats:
        base_dir = os.path.dirname(output_path)
        summary_path = os.path.join(base_dir, "user_frequency_summary_by_assigned_random.csv")
        dist_path = os.path.join(base_dir, "user_frequency_distribution_by_assigned_random.csv")
        summary.to_csv(summary_path, index=False)
        dist.to_csv(dist_path, index=False)
        print(f"\nğŸ“„ Stats saved:\n- {summary_path}\n- {dist_path}")
